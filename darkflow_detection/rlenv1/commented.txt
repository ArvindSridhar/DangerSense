# print(env.reset())
# print(env.step(1))
# print(env.step(0))

# Reward matrix:
# State 1: dot outside
# 	Stay: +100
# 	Move down: -50
# 	Move left: -100
# 	Move right: -100
# State 2: dot inside, not big enough
# 	Stay: -100
# 	Move down: +100
# 	Move left: +20
# 	Move right: +20
# State 3: dot inside, big (accident about to happen)
# 	Stay: -100
# 	Move down: +50
# 	Move left: +100
# 	Move right: +100


# def naive_sum_reward_agent(env, num_episodes=1000):
# 	# this is the table that will hold our summated rewards for
# 	# each action in each state
# 	r_table = np.zeros((5, 2))
# 	for g in range(num_episodes):
# 		s = env.reset()
# 		done = False
# 		while not done:
# 			if np.sum(r_table[s, :]) == 0:
# 				# make a random selection of actions
# 				a = np.random.randint(0, 2)
# 			else:
# 				# select the action with highest cummulative reward
# 				a = np.argmax(r_table[s, :])
# 			new_s, r, done, _ = env.step(a)
# 			r_table[s, a] += r
# 			s = new_s
# 	return r_table

# def q_learning_with_table(env, num_episodes=1000):
# 	q_table = np.zeros((5, 2))
# 	y = 0.95
# 	lr = 0.8
# 	for i in range(num_episodes):
# 		s = env.reset()
# 		done = False
# 		while not done:
# 			if np.sum(q_table[s,:]) == 0:
# 				# make a random selection of actions
# 				a = np.random.randint(0, 2)
# 			else:
# 				# select the action with largest q value in state s
# 				a = np.argmax(q_table[s, :])
# 			new_s, r, done, _ = env.step(a)
# 			q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])
# 			s = new_s
# 	return q_table

# def eps_greedy_q_learning_with_table(env, num_episodes=1000):
# 	q_table = np.zeros((5, 2))
# 	y = 0.95
# 	eps = 0.5
# 	lr = 0.8
# 	decay_factor = 0.999
# 	for i in range(num_episodes):
# 		s = env.reset()
# 		eps *= decay_factor
# 		done = False
# 		while not done:
# 			# select the action with highest cummulative reward
# 			if np.random.random() < eps or np.sum(q_table[s, :]) == 0:
# 				a = np.random.randint(0, 2)
# 			else:
# 				a = np.argmax(q_table[s, :])
# 			# pdb.set_trace()
# 			new_s, r, done, _ = env.step(a)
# 			q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])
# 			s = new_s
# 	return q_table

# # print(eps_greedy_q_learning_with_table(env))

# model = Sequential()
# # model.add(InputLayer(batch_input_shape=(1, 4)))
# model.add(Dense(16, activation='relu', input_shape=(4,)))
# model.add(Dense(16, activation='relu'))
# model.add(Dense(4, activation='softmax'))
# model.compile(loss='mse', optimizer='adam', metrics=['mae'])
# model.summary()

# if s == 2 and a in [0, 2]:
#     print(model.predict(np.identity(4)[s:s + 1]))



# self.action0changes = { # maintain speed
# 	0: [100, 0],
# 	1: [50, 1],
# 	2: [-20000, 3],
# 	3: [0, 0],
# }
# self.action1changes = { # decrease speed
# 	0: [-50, 0],
# 	1: [20, 1], # might be 0
# 	2: [75, 1],
# 	3: [0, 0],
# }
# self.action2changes = { # increase speed
# 	0: [30, 0],
# 	1: [-50, 2],
# 	2: [-20000, 3],
# 	3: [0, 0],
# }
# self.action3changes = { # swerve
# 	0: [-100, 1],
# 	1: [-100, 0],
# 	2: [100, 0],
# 	3: [0, 0],
# }
# self.actiondescs = {
# 	0: "maintain speed",
# 	1: "decrease speed",
# 	2: "increase speed",
# 	3: "swerve",
# }

# self.action0changes = { # maintain speed
# 	0: [10, 0],
# 	1: [10, 1],
# 	2: [10, 3],
# }
# self.action1changes = { # decrease speed
# 	0: [10, 0],
# 	1: [10, 1], # might be 0
# 	2: [10, 1],
# }
# self.action2changes = { # increase speed
# 	0: [10, 0],
# 	1: [10, 2],
# 	2: [10, 3],
# }
# self.action3changes = { # swerve
# 	0: [10, 1],
# 	1: [10, 0],
# 	2: [10, 0],
# }

#reward += 100
# reward /= 10
# if self.state == 3:
# 	print("previous state:", prevstate)
# 	print("going to state:", self.state)
# 	print("action:", self.actiondescs[action])
# 	print("reward:", reward)
# done = (self.state == 3)
